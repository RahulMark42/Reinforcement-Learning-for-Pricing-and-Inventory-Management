{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Fi-zZAm2_lw",
        "outputId": "665c10ae-f392-4dce-b63a-2ef8d62c82c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (0.4.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE70mPHTuIta"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(0)\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "##Importing the model (function approximator for Q-table)\n",
        "# from model import QNetwork\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn as nn\n",
        "\n",
        "from PPOAgent import PPO\n",
        "from MultiEchelonEnvironment import MultiEchelonInvOptEnv, Retailer, DistributionCenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsHazBkquItb"
      },
      "outputs": [],
      "source": [
        "demand_hist_list = []\n",
        "for k in range(4):\n",
        "    demand_hist = []\n",
        "    for i in range(52):\n",
        "        for j in range(4):\n",
        "            random_demand = np.random.normal(3, 1.5)\n",
        "            if random_demand < 0:\n",
        "                random_demand = 0\n",
        "            random_demand = np.round(random_demand)\n",
        "            demand_hist.append(random_demand)\n",
        "        random_demand = np.random.normal(6, 1)\n",
        "        if random_demand < 0:\n",
        "            random_demand = 0\n",
        "        random_demand = np.round(random_demand)\n",
        "        demand_hist.append(random_demand)\n",
        "        for j in range(2):\n",
        "            random_demand = np.random.normal(12, 2)\n",
        "            if random_demand < 0:\n",
        "                random_demand = 0\n",
        "            random_demand = np.round(random_demand)\n",
        "            demand_hist.append(random_demand)\n",
        "    demand_hist_list.append(demand_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9lkiSZbuItc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "################################### Training ###################################\n",
        "def train():\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "    has_continuous_action_space = False # continuous action space; else discrete\n",
        "\n",
        "    max_ep_len = 364                   # max timesteps in one episode\n",
        "    max_training_timesteps = int(364*15000)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "    print_freq = max_ep_len * 10        # print avg reward in the interval (in num timesteps)\n",
        "\n",
        "    action_std = 0.6            # starting std for action distribution (Multivariate Normal)\n",
        "    action_std_decay_rate = 0.03       # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
        "    min_action_std = 0.03               # minimum action_std (stop decay after action_std <= min_action_std)\n",
        "    action_std_decay_freq = int(1e5)  # action_std decay frequency (in num timesteps)\n",
        "    #####################################################\n",
        "\n",
        "    ## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "    ################ PPO hyperparameters ################\n",
        "    update_timestep = max_ep_len/2       # update policy every n timesteps\n",
        "    K_epochs = 20               # update policy for K epochs in one PPO update\n",
        "\n",
        "    eps_clip = 0.2          # clip parameter for PPO\n",
        "    gamma = 0.99            # discount factor\n",
        "    lr_actor = 0.00005       # learning rate for actor network\n",
        "    lr_critic = 0.0001       # learning rate for critic network\n",
        "\n",
        "    random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "    #####################################################\n",
        "\n",
        "    state_dim = 9\n",
        "    action_dim = 275\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    ################# training procedure ################\n",
        "\n",
        "    # initialize a PPO agent\n",
        "    ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "    # track total training time\n",
        "    start_time = datetime.now().replace(microsecond=0)\n",
        "    print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "    print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "    # printing and logging variables\n",
        "    print_running_reward = 0\n",
        "    print_running_episodes = 0\n",
        "\n",
        "    time_step = 0\n",
        "    i_episode = 0\n",
        "    i_reward = []\n",
        "    i_action = []\n",
        "    # training loop\n",
        "    for i in range(2):\n",
        "        env = MultiEchelonInvOptEnv(demand_hist_list[i*2:i*2+2])\n",
        "        while time_step <= max_training_timesteps:\n",
        "            state = env.reset()\n",
        "            current_ep_reward = 0\n",
        "            episodic_rewards = []\n",
        "            episodic_actions = []\n",
        "            for t in range(1, max_ep_len+1):\n",
        "                # select action with policy\n",
        "                action = ppo_agent.select_action(state)\n",
        "                state, reward, done = env.step(action)\n",
        "                # saving reward and is_terminals\n",
        "                ppo_agent.buffer.rewards.append(reward)\n",
        "                ppo_agent.buffer.is_terminals.append(done)\n",
        "                time_step +=1\n",
        "                current_ep_reward += reward\n",
        "                # update PPO agent\n",
        "                if time_step % update_timestep == 0:\n",
        "                    ppo_agent.update()\n",
        "                # if continuous action space; then decay action std of ouput action distribution\n",
        "                if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "                    ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "                # printing average reward\n",
        "                if time_step % print_freq == 0:\n",
        "                    # print average reward till last episode\n",
        "                    print_avg_reward = print_running_reward / print_running_episodes\n",
        "                    print_avg_reward = round(print_avg_reward, 2)\n",
        "                    print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "                    print_running_reward = 0\n",
        "                    print_running_episodes = 0\n",
        "                # break; if the episode is over\n",
        "                if done:\n",
        "                    break\n",
        "                episodic_rewards.append(current_ep_reward)\n",
        "                episodic_actions.append(action)\n",
        "            print_running_reward += current_ep_reward\n",
        "            print_running_episodes += 1\n",
        "            i_episode += 1\n",
        "            i_reward.append(episodic_rewards)\n",
        "            i_action.append(episodic_actions)\n",
        "    torch.save(ppo_agent.policy.state_dict(), 'multiechelonppo.pt')\n",
        "    return i_reward, i_action\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards, actions = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MylXYfw3zX-e",
        "outputId": "f9cfc8fe-119a-47f0-92cf-02d36ef142e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Started training at (GMT) :  2023-11-30 07:07:32\n",
            "============================================================================================\n",
            "Episode : 9 \t\t Timestep : 3640 \t\t Average Reward : -268661.89\n",
            "Episode : 19 \t\t Timestep : 7280 \t\t Average Reward : -273258.2\n",
            "Episode : 29 \t\t Timestep : 10920 \t\t Average Reward : -251446.3\n",
            "Episode : 39 \t\t Timestep : 14560 \t\t Average Reward : -222106.8\n",
            "Episode : 49 \t\t Timestep : 18200 \t\t Average Reward : -195579.6\n",
            "Episode : 59 \t\t Timestep : 21840 \t\t Average Reward : -144305.0\n",
            "Episode : 69 \t\t Timestep : 25480 \t\t Average Reward : -110802.7\n",
            "Episode : 79 \t\t Timestep : 29120 \t\t Average Reward : -93142.4\n",
            "Episode : 89 \t\t Timestep : 32760 \t\t Average Reward : -100461.1\n",
            "Episode : 99 \t\t Timestep : 36400 \t\t Average Reward : -141016.6\n",
            "Episode : 109 \t\t Timestep : 40040 \t\t Average Reward : -181956.3\n",
            "Episode : 119 \t\t Timestep : 43680 \t\t Average Reward : -206804.0\n",
            "Episode : 129 \t\t Timestep : 47320 \t\t Average Reward : -201536.2\n",
            "Episode : 139 \t\t Timestep : 50960 \t\t Average Reward : -209631.9\n",
            "Episode : 149 \t\t Timestep : 54600 \t\t Average Reward : -204765.7\n",
            "Episode : 159 \t\t Timestep : 58240 \t\t Average Reward : -199038.7\n",
            "Episode : 169 \t\t Timestep : 61880 \t\t Average Reward : -212409.9\n",
            "Episode : 179 \t\t Timestep : 65520 \t\t Average Reward : -194276.8\n",
            "Episode : 189 \t\t Timestep : 69160 \t\t Average Reward : -166130.9\n",
            "Episode : 199 \t\t Timestep : 72800 \t\t Average Reward : -164200.7\n",
            "Episode : 209 \t\t Timestep : 76440 \t\t Average Reward : -155619.8\n",
            "Episode : 219 \t\t Timestep : 80080 \t\t Average Reward : -176183.3\n",
            "Episode : 229 \t\t Timestep : 83720 \t\t Average Reward : -178072.1\n",
            "Episode : 239 \t\t Timestep : 87360 \t\t Average Reward : -176405.4\n",
            "Episode : 249 \t\t Timestep : 91000 \t\t Average Reward : -207234.7\n",
            "Episode : 259 \t\t Timestep : 94640 \t\t Average Reward : -216175.0\n",
            "Episode : 269 \t\t Timestep : 98280 \t\t Average Reward : -169498.4\n",
            "Episode : 279 \t\t Timestep : 101920 \t\t Average Reward : -154987.9\n",
            "Episode : 289 \t\t Timestep : 105560 \t\t Average Reward : -154970.2\n",
            "Episode : 299 \t\t Timestep : 109200 \t\t Average Reward : -133729.3\n",
            "Episode : 309 \t\t Timestep : 112840 \t\t Average Reward : -110523.9\n",
            "Episode : 319 \t\t Timestep : 116480 \t\t Average Reward : -90100.4\n",
            "Episode : 329 \t\t Timestep : 120120 \t\t Average Reward : -85168.0\n",
            "Episode : 339 \t\t Timestep : 123760 \t\t Average Reward : -97446.2\n",
            "Episode : 349 \t\t Timestep : 127400 \t\t Average Reward : -80746.0\n",
            "Episode : 359 \t\t Timestep : 131040 \t\t Average Reward : -76438.1\n",
            "Episode : 369 \t\t Timestep : 134680 \t\t Average Reward : -89002.4\n",
            "Episode : 379 \t\t Timestep : 138320 \t\t Average Reward : -83125.0\n",
            "Episode : 389 \t\t Timestep : 141960 \t\t Average Reward : -73999.8\n",
            "Episode : 399 \t\t Timestep : 145600 \t\t Average Reward : -85078.7\n",
            "Episode : 409 \t\t Timestep : 149240 \t\t Average Reward : -89422.1\n",
            "Episode : 419 \t\t Timestep : 152880 \t\t Average Reward : -85648.6\n",
            "Episode : 429 \t\t Timestep : 156520 \t\t Average Reward : -76834.5\n",
            "Episode : 439 \t\t Timestep : 160160 \t\t Average Reward : -79113.4\n",
            "Episode : 449 \t\t Timestep : 163800 \t\t Average Reward : -88615.1\n",
            "Episode : 459 \t\t Timestep : 167440 \t\t Average Reward : -97485.3\n",
            "Episode : 469 \t\t Timestep : 171080 \t\t Average Reward : -101120.4\n",
            "Episode : 479 \t\t Timestep : 174720 \t\t Average Reward : -121357.6\n",
            "Episode : 489 \t\t Timestep : 178360 \t\t Average Reward : -148500.6\n",
            "Episode : 499 \t\t Timestep : 182000 \t\t Average Reward : -179410.1\n",
            "Episode : 509 \t\t Timestep : 185640 \t\t Average Reward : -189285.2\n",
            "Episode : 519 \t\t Timestep : 189280 \t\t Average Reward : -207812.4\n",
            "Episode : 529 \t\t Timestep : 192920 \t\t Average Reward : -215698.5\n",
            "Episode : 539 \t\t Timestep : 196560 \t\t Average Reward : -187890.8\n",
            "Episode : 549 \t\t Timestep : 200200 \t\t Average Reward : -179326.0\n",
            "Episode : 559 \t\t Timestep : 203840 \t\t Average Reward : -195762.2\n",
            "Episode : 569 \t\t Timestep : 207480 \t\t Average Reward : -154849.0\n",
            "Episode : 579 \t\t Timestep : 211120 \t\t Average Reward : -117031.7\n",
            "Episode : 589 \t\t Timestep : 214760 \t\t Average Reward : -108930.9\n",
            "Episode : 599 \t\t Timestep : 218400 \t\t Average Reward : -131277.2\n",
            "Episode : 609 \t\t Timestep : 222040 \t\t Average Reward : -166212.0\n",
            "Episode : 619 \t\t Timestep : 225680 \t\t Average Reward : -177465.2\n",
            "Episode : 629 \t\t Timestep : 229320 \t\t Average Reward : -181104.2\n",
            "Episode : 639 \t\t Timestep : 232960 \t\t Average Reward : -182555.0\n",
            "Episode : 649 \t\t Timestep : 236600 \t\t Average Reward : -199165.0\n",
            "Episode : 659 \t\t Timestep : 240240 \t\t Average Reward : -178928.1\n",
            "Episode : 669 \t\t Timestep : 243880 \t\t Average Reward : -196495.1\n",
            "Episode : 679 \t\t Timestep : 247520 \t\t Average Reward : -210033.4\n",
            "Episode : 689 \t\t Timestep : 251160 \t\t Average Reward : -205386.4\n",
            "Episode : 699 \t\t Timestep : 254800 \t\t Average Reward : -194271.5\n",
            "Episode : 709 \t\t Timestep : 258440 \t\t Average Reward : -181349.0\n",
            "Episode : 719 \t\t Timestep : 262080 \t\t Average Reward : -198590.2\n",
            "Episode : 729 \t\t Timestep : 265720 \t\t Average Reward : -187642.2\n",
            "Episode : 739 \t\t Timestep : 269360 \t\t Average Reward : -183254.4\n",
            "Episode : 749 \t\t Timestep : 273000 \t\t Average Reward : -178054.3\n",
            "Episode : 759 \t\t Timestep : 276640 \t\t Average Reward : -194295.4\n",
            "Episode : 769 \t\t Timestep : 280280 \t\t Average Reward : -213871.9\n",
            "Episode : 779 \t\t Timestep : 283920 \t\t Average Reward : -215574.9\n",
            "Episode : 789 \t\t Timestep : 287560 \t\t Average Reward : -221527.3\n",
            "Episode : 799 \t\t Timestep : 291200 \t\t Average Reward : -240392.8\n",
            "Episode : 809 \t\t Timestep : 294840 \t\t Average Reward : -225768.4\n",
            "Episode : 819 \t\t Timestep : 298480 \t\t Average Reward : -234262.0\n",
            "Episode : 829 \t\t Timestep : 302120 \t\t Average Reward : -227090.6\n",
            "Episode : 839 \t\t Timestep : 305760 \t\t Average Reward : -236298.9\n",
            "Episode : 849 \t\t Timestep : 309400 \t\t Average Reward : -223157.8\n",
            "Episode : 859 \t\t Timestep : 313040 \t\t Average Reward : -223643.7\n",
            "Episode : 869 \t\t Timestep : 316680 \t\t Average Reward : -219734.9\n",
            "Episode : 879 \t\t Timestep : 320320 \t\t Average Reward : -235035.3\n",
            "Episode : 889 \t\t Timestep : 323960 \t\t Average Reward : -238723.8\n",
            "Episode : 899 \t\t Timestep : 327600 \t\t Average Reward : -239964.6\n",
            "Episode : 909 \t\t Timestep : 331240 \t\t Average Reward : -225333.4\n",
            "Episode : 919 \t\t Timestep : 334880 \t\t Average Reward : -241466.1\n",
            "Episode : 929 \t\t Timestep : 338520 \t\t Average Reward : -243672.1\n",
            "Episode : 939 \t\t Timestep : 342160 \t\t Average Reward : -242457.4\n",
            "Episode : 949 \t\t Timestep : 345800 \t\t Average Reward : -229684.0\n",
            "Episode : 959 \t\t Timestep : 349440 \t\t Average Reward : -232684.2\n",
            "Episode : 969 \t\t Timestep : 353080 \t\t Average Reward : -231332.2\n",
            "Episode : 979 \t\t Timestep : 356720 \t\t Average Reward : -236690.6\n",
            "Episode : 989 \t\t Timestep : 360360 \t\t Average Reward : -225394.3\n",
            "Episode : 999 \t\t Timestep : 364000 \t\t Average Reward : -215366.6\n",
            "Episode : 1009 \t\t Timestep : 367640 \t\t Average Reward : -133893.4\n",
            "Episode : 1019 \t\t Timestep : 371280 \t\t Average Reward : -86113.2\n",
            "Episode : 1029 \t\t Timestep : 374920 \t\t Average Reward : -83968.0\n",
            "Episode : 1039 \t\t Timestep : 378560 \t\t Average Reward : -77169.4\n",
            "Episode : 1049 \t\t Timestep : 382200 \t\t Average Reward : -83935.5\n",
            "Episode : 1059 \t\t Timestep : 385840 \t\t Average Reward : -70069.2\n",
            "Episode : 1069 \t\t Timestep : 389480 \t\t Average Reward : -62924.7\n",
            "Episode : 1079 \t\t Timestep : 393120 \t\t Average Reward : -34267.8\n",
            "Episode : 1089 \t\t Timestep : 396760 \t\t Average Reward : -18757.7\n",
            "Episode : 1099 \t\t Timestep : 400400 \t\t Average Reward : -8320.7\n",
            "Episode : 1109 \t\t Timestep : 404040 \t\t Average Reward : -8101.1\n",
            "Episode : 1119 \t\t Timestep : 407680 \t\t Average Reward : -8028.7\n",
            "Episode : 1129 \t\t Timestep : 411320 \t\t Average Reward : -2590.3\n",
            "Episode : 1139 \t\t Timestep : 414960 \t\t Average Reward : -7033.5\n",
            "Episode : 1149 \t\t Timestep : 418600 \t\t Average Reward : 1710.6\n",
            "Episode : 1159 \t\t Timestep : 422240 \t\t Average Reward : 1942.4\n",
            "Episode : 1169 \t\t Timestep : 425880 \t\t Average Reward : 1988.0\n",
            "Episode : 1179 \t\t Timestep : 429520 \t\t Average Reward : 2885.3\n",
            "Episode : 1189 \t\t Timestep : 433160 \t\t Average Reward : 3944.4\n",
            "Episode : 1199 \t\t Timestep : 436800 \t\t Average Reward : 3215.1\n",
            "Episode : 1209 \t\t Timestep : 440440 \t\t Average Reward : 3240.2\n",
            "Episode : 1219 \t\t Timestep : 444080 \t\t Average Reward : 246.2\n",
            "Episode : 1229 \t\t Timestep : 447720 \t\t Average Reward : 1211.5\n",
            "Episode : 1239 \t\t Timestep : 451360 \t\t Average Reward : 2517.3\n",
            "Episode : 1249 \t\t Timestep : 455000 \t\t Average Reward : 248.0\n",
            "Episode : 1259 \t\t Timestep : 458640 \t\t Average Reward : 21.5\n",
            "Episode : 1269 \t\t Timestep : 462280 \t\t Average Reward : -1969.3\n",
            "Episode : 1279 \t\t Timestep : 465920 \t\t Average Reward : 3149.2\n",
            "Episode : 1289 \t\t Timestep : 469560 \t\t Average Reward : 2094.5\n",
            "Episode : 1299 \t\t Timestep : 473200 \t\t Average Reward : 4119.7\n",
            "Episode : 1309 \t\t Timestep : 476840 \t\t Average Reward : 348.4\n",
            "Episode : 1319 \t\t Timestep : 480480 \t\t Average Reward : 3503.9\n",
            "Episode : 1329 \t\t Timestep : 484120 \t\t Average Reward : 2764.5\n",
            "Episode : 1339 \t\t Timestep : 487760 \t\t Average Reward : 3644.3\n",
            "Episode : 1349 \t\t Timestep : 491400 \t\t Average Reward : 2307.9\n",
            "Episode : 1359 \t\t Timestep : 495040 \t\t Average Reward : 4337.6\n",
            "Episode : 1369 \t\t Timestep : 498680 \t\t Average Reward : 4273.6\n",
            "Episode : 1379 \t\t Timestep : 502320 \t\t Average Reward : 3993.1\n",
            "Episode : 1389 \t\t Timestep : 505960 \t\t Average Reward : 4450.9\n",
            "Episode : 1399 \t\t Timestep : 509600 \t\t Average Reward : 4454.5\n",
            "Episode : 1409 \t\t Timestep : 513240 \t\t Average Reward : 4120.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_episodic_rewards = []\n",
        "\n",
        "for i in range(len(rewards)):\n",
        "  ppo_episodic_rewards.append(np.mean(rewards[i]))"
      ],
      "metadata": {
        "id": "O8o-VjVlDFV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(ppo_episodic_rewards, color = 'orange')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Episodic Rewards during training')"
      ],
      "metadata": {
        "id": "G3q7jhPt031p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0KJfH08uItg"
      },
      "outputs": [],
      "source": [
        "def MultiEchelonInvOpt_sS(s_DC,S_DC,s_r1,S_r1,s_r2,S_r2):\n",
        "    if s_DC > S_DC-1 or s_r1 > S_r1-1 or s_r2 > S_r2-1:\n",
        "        return -1e8\n",
        "    else:\n",
        "        n_retailers = 4\n",
        "        n_DCs = 2\n",
        "        retailers = []\n",
        "        for i in range(n_retailers):\n",
        "            retailers.append(Retailer(demand_hist_list[i]))\n",
        "        DCs = []\n",
        "        for i in range(n_DCs):\n",
        "            DCs.append(DistributionCenter())\n",
        "        n_period = len(demand_hist_list[0])\n",
        "        variable_order_cost = 10\n",
        "        current_period = 1\n",
        "        total_reward = 0\n",
        "        while current_period <= n_period:\n",
        "            action = []\n",
        "            for DC in DCs:\n",
        "                if DC.inv_pos <= s_DC:\n",
        "                    action.append(np.round(min(DC.order_quantity_limit,S_DC-DC.inv_pos)))\n",
        "                else:\n",
        "                    action.append(0)\n",
        "            for i in range(len(retailers)):\n",
        "                if i%2 == 0:\n",
        "                    if retailers[i].inv_pos <= s_r1:\n",
        "                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r1-retailers[i].inv_pos)))\n",
        "                    else:\n",
        "                        action.append(0)\n",
        "                else:\n",
        "                    if retailers[i].inv_pos <= s_r2:\n",
        "                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r2-retailers[i].inv_pos)))\n",
        "                    else:\n",
        "                        action.append(0)\n",
        "            y_list = []\n",
        "            for i in range(n_DCs):\n",
        "                y = 1 if action[i] > 0 else 0\n",
        "                y_list.append(y)\n",
        "            for DC,order_quantity in zip(DCs,action[:n_DCs]):\n",
        "                DC.place_order(order_quantity,current_period)\n",
        "            sum_holding_cost_DC = 0\n",
        "            for i in range(n_DCs):\n",
        "                holding_cost_total = DCs[i].order_arrival(retailers[i*2:i*2+2],current_period)\n",
        "                sum_holding_cost_DC += holding_cost_total\n",
        "                DCs[i].satisfy_demand(retailers[i*2:i*2+2],action[i*2+2:i*2+4],current_period)\n",
        "            sum_n_orders = 0\n",
        "            sum_holding_cost_retailer = 0\n",
        "            sum_revenue = 0\n",
        "            for retailer,demand in zip(retailers,demand_hist_list):\n",
        "                n_orders, holding_cost_total = retailer.order_arrival(current_period)\n",
        "                sum_n_orders += n_orders\n",
        "                sum_holding_cost_retailer += holding_cost_total\n",
        "                revenue = retailer.satisfy_demand(demand[current_period-1])\n",
        "                sum_revenue += revenue\n",
        "            reward = sum_revenue - sum_holding_cost_retailer - sum_holding_cost_DC - sum_n_orders*retailers[0].fixed_order_cost - \\\n",
        "                     np.sum(y_list)*DCs[0].fixed_order_cost - np.sum(action[:n_DCs])*variable_order_cost\n",
        "\n",
        "            current_period += 1\n",
        "            total_reward += reward\n",
        "        return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzNKa_DjuItg"
      },
      "outputs": [],
      "source": [
        "# from bayes_opt import BayesianOptimization\n",
        "# pbounds = {'s_DC': (0,210), 'S_DC': (0, 210), 's_r1': (0, 90), 'S_r1': (0, 90), 's_r2': (0, 90), 'S_r2': (0, 90)}\n",
        "# optimizer = BayesianOptimization(\n",
        "#     f=MultiEchelonInvOpt_sS,\n",
        "#     pbounds=pbounds,\n",
        "#     random_state=0,\n",
        "# )\n",
        "# optimizer.maximize(\n",
        "#     init_points = 100,\n",
        "#     n_iter=1000\n",
        "# )\n",
        "# print(optimizer.max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIgylQ8YuItg"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "demand_test = []\n",
        "for k in range(100,200):\n",
        "    demand_list = []\n",
        "    for k in range(4):\n",
        "        demand = []\n",
        "        for i in range(52):\n",
        "            for j in range(4):\n",
        "                random_demand = np.random.normal(3, 1.5)\n",
        "                if random_demand < 0:\n",
        "                    random_demand = 0\n",
        "                random_demand = np.round(random_demand)\n",
        "                demand.append(random_demand)\n",
        "            random_demand = np.random.normal(6, 1)\n",
        "            if random_demand < 0:\n",
        "                random_demand = 0\n",
        "            random_demand = np.round(random_demand)\n",
        "            demand.append(random_demand)\n",
        "            for j in range(2):\n",
        "                random_demand = np.random.normal(12, 2)\n",
        "                if random_demand < 0:\n",
        "                    random_demand = 0\n",
        "                random_demand = np.round(random_demand)\n",
        "                demand.append(random_demand)\n",
        "        demand_list.append(demand)\n",
        "    demand_test.append(demand_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWuKDcdfuIth"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "has_continuous_action_space = False # continuous action space; else discrete\n",
        "action_std = 0.6            # starting std for action distribution (Multivariate Normal)\n",
        "action_std_decay_rate = 0.03       # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
        "min_action_std = 0.03               # minimum action_std (stop decay after action_std <= min_action_std)\n",
        "action_std_decay_freq = int(1e5)  # action_std decay frequency (in num timesteps\n",
        "eps_clip = 0.2          # clip parameter for PPO\n",
        "gamma = 0.99            # discount factor\n",
        "K_epochs = 20\n",
        "lr_actor = 0.00005      # learning rate for actor network\n",
        "lr_critic = 0.0001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "#####################################################\n",
        "\n",
        "state_dim = 9\n",
        "action_dim = 275\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "#ppo_agent.policy_old.load_state_dict(torch.load(desired_path))\n",
        "#ppo_agent.policy.load_state_dict(torch.load(desired_path))\n",
        "\n",
        "reward_RL = []\n",
        "for demand in demand_test:\n",
        "    reward_total = 0\n",
        "    for i in range(2):\n",
        "        env = MultiEchelonInvOptEnv(demand[i*2:i*2+2])\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        reward_sub = 0\n",
        "        while not done:\n",
        "            action = ppo_agent.select_action(state)\n",
        "            state, reward, done = env.step(action)\n",
        "            reward_sub += reward\n",
        "            if done:\n",
        "                break\n",
        "        reward_total += reward_sub\n",
        "    reward_RL.append(reward_total)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(ppo_agent.policy.state_dict(), 'multiechelonppo.pt')"
      ],
      "metadata": {
        "id": "bgb8oUxfvlVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdzVVihuuIth"
      },
      "outputs": [],
      "source": [
        "def MultiEchelonInvOpt_sS_test(s_DC,S_DC,s_r1,S_r1,s_r2,S_r2,demand_records):\n",
        "    if s_DC > S_DC-1 or s_r1 > S_r1-1 or s_r2 > S_r2-1:\n",
        "        return -1e8\n",
        "    else:\n",
        "        n_retailers = 4\n",
        "        n_DCs = 2\n",
        "        retailers = []\n",
        "        for i in range(n_retailers):\n",
        "            retailers.append(Retailer(demand_records[i]))\n",
        "        DCs = []\n",
        "        for i in range(n_DCs):\n",
        "            DCs.append(DistributionCenter())\n",
        "        n_period = len(demand_records[0])\n",
        "        variable_order_cost = 10\n",
        "        current_period = 1\n",
        "        total_reward = 0\n",
        "        total_revenue = 0\n",
        "        total_holding_cost_retailer = 0\n",
        "        total_holding_cost_DC = 0\n",
        "        total_variable_cost = 0\n",
        "        while current_period <= n_period:\n",
        "            action = []\n",
        "            for DC in DCs:\n",
        "                if DC.inv_pos <= s_DC:\n",
        "                    action.append(np.round(min(DC.order_quantity_limit,S_DC-DC.inv_pos)))\n",
        "                else:\n",
        "                    action.append(0)\n",
        "            for i in range(len(retailers)):\n",
        "                if i%2 == 0:\n",
        "                    if retailers[i].inv_pos <= s_r1:\n",
        "                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r1-retailers[i].inv_pos)))\n",
        "                    else:\n",
        "                        action.append(0)\n",
        "                else:\n",
        "                    if retailers[i].inv_pos <= s_r2:\n",
        "                        action.append(np.round(min(retailers[i].order_quantity_limit,S_r2-retailers[i].inv_pos)))\n",
        "                    else:\n",
        "                        action.append(0)\n",
        "            y_list = []\n",
        "            for i in range(n_DCs):\n",
        "                y = 1 if action[i] > 0 else 0\n",
        "                y_list.append(y)\n",
        "            for DC,order_quantity in zip(DCs,action[:n_DCs]):\n",
        "                DC.place_order(order_quantity,current_period)\n",
        "            sum_holding_cost_DC = 0\n",
        "            for i in range(n_DCs):\n",
        "                holding_cost_total = DCs[i].order_arrival(retailers[i*2:i*2+2],current_period)\n",
        "                sum_holding_cost_DC += holding_cost_total\n",
        "                DCs[i].satisfy_demand(retailers[i*2:i*2+2],action[i*2+2:i*2+4],current_period)\n",
        "\n",
        "            sum_n_orders = 0\n",
        "            sum_holding_cost_retailer = 0\n",
        "            sum_revenue = 0\n",
        "            for retailer,demand in zip(retailers,demand_records):\n",
        "                n_orders, holding_cost_total = retailer.order_arrival(current_period)\n",
        "                sum_n_orders += n_orders\n",
        "                sum_holding_cost_retailer += holding_cost_total\n",
        "                revenue = retailer.satisfy_demand(demand[current_period-1])\n",
        "                sum_revenue += revenue\n",
        "            reward = sum_revenue - sum_holding_cost_retailer - sum_holding_cost_DC - sum_n_orders*retailers[0].fixed_order_cost - \\\n",
        "                     np.sum(y_list)*DCs[0].fixed_order_cost - np.sum(action[:n_DCs])*variable_order_cost\n",
        "\n",
        "            current_period += 1\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMESkv1tuIth"
      },
      "outputs": [],
      "source": [
        "reward_sS = []\n",
        "for demand in demand_test:\n",
        "    reward = MultiEchelonInvOpt_sS_test(62, 120, 15, 41, 17, 90, demand)\n",
        "    reward_sS.append(reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujPdEB3puIth"
      },
      "outputs": [],
      "source": [
        "plt.boxplot([reward_sS, -0.1*np.array(reward_RL)], labels = ['S,s','PPO'])\n",
        "plt.ylabel('Profit')\n",
        "plt.title('Comparing Profits by Different Algorithms')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJMpme3x1ZCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}